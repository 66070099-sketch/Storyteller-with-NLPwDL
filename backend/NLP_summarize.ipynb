{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp==5.0.4 scikit-learn==1.5.1 regex==2024.7.24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Eq8CAioV3gWR",
        "outputId": "aa58f178-e223-44b6-cc69-d2cf4037bd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp==5.0.4\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting scikit-learn==1.5.1\n",
            "  Downloading scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting regex==2024.7.24\n",
            "  Downloading regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from pythainlp==5.0.4) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->pythainlp==5.0.4) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->pythainlp==5.0.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->pythainlp==5.0.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->pythainlp==5.0.4) (2025.8.3)\n",
            "Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (790 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m790.9/790.9 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: regex, scikit-learn, pythainlp\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pythainlp-5.0.4 regex-2024.7.24 scikit-learn-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-crfsuite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L_t75XfS35fI",
        "outputId": "eb23fadd-ccf9-49b9-95c0-5e90b565f01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m1.2/1.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ ‡πÅ‡∏õ‡∏£‡∏á‡πÑ‡∏ü‡∏•‡πåtext > json ===========================\n",
        "import os\n",
        "import json\n",
        "\n",
        "# üìÇ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå .txt\n",
        "INPUT_DIR = \"/content/drive/MyDrive/Tales40\"   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô path ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "OUTPUT_JSON = \"/content/1-40stories_fixed.json\"\n",
        "\n",
        "# üîΩ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ key = \"stories\"\n",
        "all_data = {\"stories\": []}\n",
        "\n",
        "# üîç loop ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå .txt ‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n",
        "for filename in sorted(os.listdir(INPUT_DIR)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(INPUT_DIR, filename)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read().strip()\n",
        "\n",
        "        story = {\n",
        "            \"title\": os.path.splitext(filename)[0],  # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô title\n",
        "            \"content\": content\n",
        "        }\n",
        "        all_data[\"stories\"].append(story)\n",
        "\n",
        "# üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà: {OUTPUT_JSON}\")\n"
      ],
      "metadata": {
        "id": "iiuqEiOtPCAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e591e24-2c3c-40f2-b50b-fb97a08ee839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà: /content/stories.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 1-20 ====================================\n",
        "\n",
        "import os, re, json, time, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from collections import Counter\n",
        "\n",
        "# ---------- Thai NLP ----------\n",
        "from pythainlp.tokenize import sent_tokenize, word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "# ---------- TF-IDF ----------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ---------- LLM (optional) ----------\n",
        "USE_LLM = True\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "OPENAI_API_KEY = \"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\"  # ‡∏ï‡∏±‡πâ‡∏á ENV ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "\n",
        "if USE_LLM and not OPENAI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö OPENAI_API_KEY ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°: ‡∏à‡∏∞‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Rule-based ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\")\n",
        "    USE_LLM = False\n",
        "\n",
        "# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö Rule-based ----------\n",
        "TH_STOP = set(thai_stopwords())\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "\n",
        "def split_sentences_th(text: str) -> List[str]:\n",
        "    sents = [s.strip() for s in sent_tokenize(text, engine=\"whitespace+newline\")]\n",
        "    out = []\n",
        "    for s in sents:\n",
        "        parts = re.split(r'([?!]|[.„ÄÇ‚Ä¶]|[‚Ä¶])', s)\n",
        "        if len(parts) > 1:\n",
        "            for i in range(0, len(parts), 2):\n",
        "                seg = parts[i].strip()\n",
        "                end = parts[i+1] if i+1 < len(parts) else \"\"\n",
        "                if seg:\n",
        "                    out.append((seg + end).strip())\n",
        "        else:\n",
        "            if s:\n",
        "                out.append(s)\n",
        "    return [s for s in out if s and not s.isspace()]\n",
        "\n",
        "def tokenize_for_keywords(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text, engine=\"newmm\")\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        if re.fullmatch(r'[0-9\\W_]+', t):\n",
        "            continue\n",
        "        if t in TH_STOP:\n",
        "            continue\n",
        "        cleaned.append(t)\n",
        "    return cleaned\n",
        "\n",
        "def chunk_indices(n_sents: int, n_chunks: int = 6) -> List[Tuple[int, int]]:\n",
        "    borders = [0] + [int(round(i * n_sents / n_chunks)) for i in range(1, n_chunks)] + [n_sents]\n",
        "    chunks = [(borders[i], borders[i+1]) for i in range(len(borders)-1)]\n",
        "    non_empty = [(s, e) for (s, e) in chunks if e - s > 0]\n",
        "    return non_empty or [(0, n_sents)]\n",
        "\n",
        "def tfidf_ranked_sentences(sub_sents: List[str], topk: int = 2) -> List[str]:\n",
        "    if not sub_sents:\n",
        "        return []\n",
        "    vec = TfidfVectorizer(tokenizer=tokenize_for_keywords, ngram_range=(1,2), min_df=1)\n",
        "    X = vec.fit_transform(sub_sents)\n",
        "    scores = (X.sum(axis=1)).A.ravel()\n",
        "    idx = np.argsort(-scores)[:max(1, topk)]\n",
        "    return [sub_sents[i].strip() for i in idx]\n",
        "\n",
        "def ensure_1_to_3_sentences(sub_sents: List[str]) -> str:\n",
        "    if not sub_sents:\n",
        "        return \"\"\n",
        "    ranked = tfidf_ranked_sentences(sub_sents, topk=2)\n",
        "    picked = [sub_sents[0].strip()]\n",
        "    for s in ranked:\n",
        "        if s not in picked:\n",
        "            picked.append(s)\n",
        "        if len(picked) >= 3:\n",
        "            break\n",
        "    if len(picked) < 2 and len(sub_sents) > 1:\n",
        "        last = sub_sents[-1].strip()\n",
        "        if last not in picked:\n",
        "            picked.append(last)\n",
        "    picked = picked[:3]\n",
        "    picked = [re.sub(r'\\s+', ' ', p).strip() for p in picked if p]\n",
        "    return \" \".join(picked)\n",
        "\n",
        "def scene_title_general(chunk_text: str) -> str:\n",
        "    toks = word_tokenize(chunk_text, engine=\"newmm\")\n",
        "    tags = pos_tag(toks, corpus=\"orchid\")\n",
        "    verbs = [w for (w, p) in tags if p.startswith(\"V\") and w not in TH_STOP]\n",
        "    if any(w in chunk_text for w in {\"‡∏•‡∏á‡πÇ‡∏ó‡∏©\",\"‡∏™‡∏≤‡∏õ\",\"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"}): return \"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"\n",
        "    if any(w in chunk_text for w in {\"‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á\",\"‡πÑ‡∏õ‡∏´‡∏≤\",\"‡πÑ‡∏õ‡∏û‡∏ö\",\"‡∏Ç‡∏∂‡πâ‡∏ô‡∏ü‡πâ‡∏≤\",\"‡∏™‡∏≤‡∏¢‡∏£‡∏∏‡πâ‡∏á\"}): return \"‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏´‡∏≤\"\n",
        "    if any(w in chunk_text for w in {\"‡πÄ‡∏à‡∏£‡∏à‡∏≤\",\"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢\",\"‡∏ß‡∏¥‡∏á‡∏ß‡∏≠‡∏ô\",\"‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á\",\"‡∏™‡∏±‡∏ç‡∏ç‡∏≤\"}): return \"‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏à‡∏≤\"\n",
        "    if any(w in chunk_text for w in {\"‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\",\"‡πÄ‡∏´‡∏•‡πá‡∏Å‡πÉ‡∏ô\",\"‡∏°‡∏≠‡∏ö\",\"‡∏™‡∏£‡πâ‡∏≤‡∏á\",\"‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\"}): return \"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\"\n",
        "    if any(w in chunk_text for w in {\"‡∏•‡∏≥‡∏û‡∏≠‡∏á\",\"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à\",\"‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\",\"‡∏£‡∏∏‡∏°\"}): return \"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\"\n",
        "    if any(w in chunk_text for w in {\"‡∏Ç‡πÇ‡∏°‡∏¢\",\"‡∏õ‡∏•‡πâ‡∏ô\",\"‡∏ï‡∏±‡∏Å‡∏ô‡πâ‡∏≥‡∏ú‡∏∂‡πâ‡∏á\"}): return \"‡∏£‡∏±‡∏á‡∏ú‡∏∂‡πâ‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏•‡πâ‡∏ô\"\n",
        "    if verbs:\n",
        "        v = verbs[0]\n",
        "        return \"‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô\" if v.startswith(\"‡∏ï‡πà‡∏≠‡∏¢\") else f\"‡∏Å‡∏≤‡∏£{v}\"\n",
        "    kws = Counter(tokenize_for_keywords(chunk_text)).most_common(1)\n",
        "    return kws[0][0] if kws else \"‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\"\n",
        "\n",
        "def moral_line(text: str) -> str:\n",
        "    sents = split_sentences_th(text)\n",
        "    for s in sents:\n",
        "        if any(c in s for c in (\"‡∏™‡∏≠‡∏ô‡∏ß‡πà‡∏≤\",\"‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\",\"‡∏Ñ‡∏ï‡∏¥\",\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏¥‡∏î\")):\n",
        "            return re.sub(r'^\\W+|\\W+$', '', s)\n",
        "    if any(w in text for w in {\"‡∏≠‡∏≥‡∏ô‡∏≤‡∏à\",\"‡∏•‡∏≥‡∏û‡∏≠‡∏á\",\"‡πÉ‡∏ä‡πâ‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\",\"‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î\"}):\n",
        "        return \"‡πÅ‡∏°‡πâ‡∏°‡∏µ‡∏û‡∏•‡∏±‡∏á ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏±‡∏ö‡∏¢‡∏±‡πâ‡∏á‡∏ä‡∏±‡πà‡∏á‡πÉ‡∏à ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≤‡∏•‡∏∞‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö\"\n",
        "    return \"‡∏à‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏≠‡∏î‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à\"\n",
        "\n",
        "def summarize_rule_based(text: str) -> str:\n",
        "    text = normalize_text(text)\n",
        "    sents = split_sentences_th(text)\n",
        "    if not sents:\n",
        "        return \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ\"\n",
        "    chunks = chunk_indices(len(sents), 6)\n",
        "    scenes = []\n",
        "    for i, (st, en) in enumerate(chunks, 1):\n",
        "        sub = sents[st:en]\n",
        "        body = ensure_1_to_3_sentences(sub)\n",
        "        title = scene_title_general(\" \".join(sub)).strip(\" Ôºå,.;:!? -‚Äî\")\n",
        "        scenes.append(f\"**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà {i}: {title}**\\n{body}\\n\")\n",
        "    if len(scenes) < 6:\n",
        "        for j in range(len(scenes)+1, 7):\n",
        "            scenes.append(f\"**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà {j}: ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**\\n(‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)\\n\")\n",
        "    moral = moral_line(text)\n",
        "    return \"\\n* * *\\n\\n\".join(scenes[:6]) + f\"\\n\\n* * *\\n\\n‡∏Ñ‡∏ï‡∏¥‡∏™‡∏≠‡∏ô‡πÉ‡∏à: **{moral}**\"\n",
        "\n",
        "# ---------- LLM prompt ----------\n",
        "def build_prompt(text: str) -> str:\n",
        "    return f\"\"\"\n",
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û\n",
        "‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô \"6 ‡∏â‡∏≤‡∏Å\" (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô/‡∏Ç‡∏≤‡∏î 6 ‡∏â‡∏≤‡∏Å)\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:\n",
        "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏â‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ **2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏ï‡πá‡∏°** ‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 2 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3\n",
        "- ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö (1) ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå (2) ‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (3) ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå/‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
        "- ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
        "- ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ ‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô \"‡∏£‡∏±‡∏á‡∏ú‡∏∂‡πâ‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏•‡πâ‡∏ô\", \"‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏´‡∏≤\", \"‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏à‡∏≤\", \"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\", \"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\", \"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"\n",
        "- ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏• ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏õ‡∏µ/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô/‡∏ß‡∏±‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏\n",
        "\n",
        "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô):\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 1: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 2: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 3: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 4: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 5: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 6: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "‡∏Ñ‡∏ï‡∏¥‡∏™‡∏≠‡∏ô‡πÉ‡∏à: <1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô>\n",
        "\n",
        "‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°:\n",
        "{text}\n",
        "\"\"\".strip()\n",
        "\n",
        "def summarize_llm(text: str, temperature: float = 0.5) -> str:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    prompt = build_prompt(text)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏ñ‡∏µ‡∏û‡∏¥‡∏ñ‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def summarize_story(text: str) -> str:\n",
        "    if USE_LLM:\n",
        "        try:\n",
        "            return summarize_llm(text)\n",
        "        except Exception as e:\n",
        "\n",
        "            return summarize_rule_based(text) + f\"\\n\\n(‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ä‡πâ Rule-based ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ LLM error: {e})\"\n",
        "    else:\n",
        "        return summarize_rule_based(text)\n",
        "\n",
        "# ---------- Batch I/O ----------\n",
        "def load_stories(json_path: str) -> List[Dict[str, Any]]:\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, dict) and \"stories\" in data and isinstance(data[\"stories\"], list):\n",
        "        items = data[\"stories\"]\n",
        "    elif isinstance(data, list):\n",
        "        items = data\n",
        "    else:\n",
        "        raise ValueError(\"‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ key 'stories'\")\n",
        "\n",
        "    norm = []\n",
        "    for i, it in enumerate(items):\n",
        "        text = it.get(\"text\") or it.get(\"content\") or it.get(\"story\") or \"\"\n",
        "        if not text or not str(text).strip():\n",
        "            continue\n",
        "        norm.append({\n",
        "            \"id\": it.get(\"id\", f\"story_{i+1:04d}\"),\n",
        "            \"title\": it.get(\"title\", \"\"),\n",
        "            \"text\": str(text),\n",
        "            **{k: v for k, v in it.items() if k not in {\"id\",\"title\",\"text\"}},\n",
        "        })\n",
        "    if not norm:\n",
        "        raise ValueError(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ü‡∏¥‡∏•‡∏î‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (text/content/story) ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏î‡πÄ‡∏•‡∏¢\")\n",
        "    return norm\n",
        "\n",
        "def batch_summarize_json(input_json: str,\n",
        "                         out_json: str = \"summaries.json\",\n",
        "                         out_csv: str = \"summaries.csv\",\n",
        "                         sleep_sec: float = 0.0) -> pd.DataFrame:\n",
        "    items = load_stories(input_json)\n",
        "    results = []\n",
        "    for idx, it in enumerate(items, 1):\n",
        "        story_id = it[\"id\"]\n",
        "        title = it.get(\"title\", \"\")\n",
        "        text  = it[\"text\"]\n",
        "        print(f\"[{idx}/{len(items)}] summarizing: {story_id} {('('+title+')') if title else ''}\")\n",
        "        summary = summarize_story(text)\n",
        "        results.append({\n",
        "            \"id\": story_id,\n",
        "            \"title\": title,\n",
        "            \"summary_6scenes\": summary\n",
        "        })\n",
        "        if sleep_sec > 0:\n",
        "            time.sleep(sleep_sec)\n",
        "    # save JSON\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    # save CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"‚úÖ Done. Saved: {out_json}, {out_csv}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_path = \"/content/drive/MyDrive/NLP_DL data/1-20stories_fixed.json\"       # <-- ‡πÉ‡∏™‡πà path ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "    out_json  = \"1-20summaries.json\"\n",
        "    out_csv   = \"1-20summaries.csv\"\n",
        "\n",
        "    df = batch_summarize_json(input_path, out_json, out_csv, sleep_sec=0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtWc8tAwHch0",
        "outputId": "f5fd79bd-f92e-4d27-a787-8fb84ec8edc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/20] summarizing: story_0001 (tale1)\n",
            "[2/20] summarizing: story_0002 (tale2)\n",
            "[3/20] summarizing: story_0003 (tale3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4/20] summarizing: story_0004 (tale4)\n",
            "[5/20] summarizing: story_0005 (tale5)\n",
            "[6/20] summarizing: story_0006 (tale6)\n",
            "[7/20] summarizing: story_0007 (tale7)\n",
            "[8/20] summarizing: story_0008 (tale8)\n",
            "[9/20] summarizing: story_0009 (tale9)\n",
            "[10/20] summarizing: story_0010 (tale10)\n",
            "[11/20] summarizing: story_0011 (tale11)\n",
            "[12/20] summarizing: story_0012 (tale12)\n",
            "[13/20] summarizing: story_0013 (tale13)\n",
            "[14/20] summarizing: story_0014 (tale14)\n",
            "[15/20] summarizing: story_0015 (tale15)\n",
            "[16/20] summarizing: story_0016 (tale16)\n",
            "[17/20] summarizing: story_0017 (tale17)\n",
            "[18/20] summarizing: story_0018 (tale18)\n",
            "[19/20] summarizing: story_0019 (tale19)\n",
            "[20/20] summarizing: story_0020 (tale20)\n",
            "‚úÖ Done. Saved: summaries.json, summaries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 21-40 ====================================\n",
        "\n",
        "import os, re, json, time, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from collections import Counter\n",
        "\n",
        "# ---------- Thai NLP ----------\n",
        "from pythainlp.tokenize import sent_tokenize, word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "# ---------- TF-IDF ----------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ---------- LLM (optional) ----------\n",
        "USE_LLM = True\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "OPENAI_API_KEY = \"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\"  # ‡∏ï‡∏±‡πâ‡∏á ENV ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "\n",
        "if USE_LLM and not OPENAI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö OPENAI_API_KEY ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°: ‡∏à‡∏∞‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Rule-based ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\")\n",
        "    USE_LLM = False\n",
        "\n",
        "# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö Rule-based ----------\n",
        "TH_STOP = set(thai_stopwords())\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "\n",
        "def split_sentences_th(text: str) -> List[str]:\n",
        "    sents = [s.strip() for s in sent_tokenize(text, engine=\"whitespace+newline\")]\n",
        "    out = []\n",
        "    for s in sents:\n",
        "        parts = re.split(r'([?!]|[.„ÄÇ‚Ä¶]|[‚Ä¶])', s)\n",
        "        if len(parts) > 1:\n",
        "            for i in range(0, len(parts), 2):\n",
        "                seg = parts[i].strip()\n",
        "                end = parts[i+1] if i+1 < len(parts) else \"\"\n",
        "                if seg:\n",
        "                    out.append((seg + end).strip())\n",
        "        else:\n",
        "            if s:\n",
        "                out.append(s)\n",
        "    return [s for s in out if s and not s.isspace()]\n",
        "\n",
        "def tokenize_for_keywords(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text, engine=\"newmm\")\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        if re.fullmatch(r'[0-9\\W_]+', t):  # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏•‡πâ‡∏ß‡∏ô\n",
        "            continue\n",
        "        if t in TH_STOP:\n",
        "            continue\n",
        "        cleaned.append(t)\n",
        "    return cleaned\n",
        "\n",
        "def chunk_indices(n_sents: int, n_chunks: int = 6) -> List[Tuple[int, int]]:\n",
        "    borders = [0] + [int(round(i * n_sents / n_chunks)) for i in range(1, n_chunks)] + [n_sents]\n",
        "    chunks = [(borders[i], borders[i+1]) for i in range(len(borders)-1)]\n",
        "    non_empty = [(s, e) for (s, e) in chunks if e - s > 0]\n",
        "    return non_empty or [(0, n_sents)]\n",
        "\n",
        "def tfidf_ranked_sentences(sub_sents: List[str], topk: int = 2) -> List[str]:\n",
        "    if not sub_sents:\n",
        "        return []\n",
        "    vec = TfidfVectorizer(tokenizer=tokenize_for_keywords, ngram_range=(1,2), min_df=1)\n",
        "    X = vec.fit_transform(sub_sents)\n",
        "    scores = (X.sum(axis=1)).A.ravel()\n",
        "    idx = np.argsort(-scores)[:max(1, topk)]\n",
        "    return [sub_sents[i].strip() for i in idx]\n",
        "\n",
        "def ensure_1_to_3_sentences(sub_sents: List[str]) -> str:\n",
        "    if not sub_sents:\n",
        "        return \"\"\n",
        "    ranked = tfidf_ranked_sentences(sub_sents, topk=2)\n",
        "    picked = [sub_sents[0].strip()]  # ‡∏Ñ‡∏∏‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤\n",
        "    for s in ranked:\n",
        "        if s not in picked:\n",
        "            picked.append(s)\n",
        "        if len(picked) >= 3:\n",
        "            break\n",
        "    if len(picked) < 2 and len(sub_sents) > 1:\n",
        "        last = sub_sents[-1].strip()\n",
        "        if last not in picked:\n",
        "            picked.append(last)\n",
        "    picked = picked[:3]\n",
        "    picked = [re.sub(r'\\s+', ' ', p).strip() for p in picked if p]\n",
        "    return \" \".join(picked)\n",
        "\n",
        "def scene_title_general(chunk_text: str) -> str:\n",
        "    toks = word_tokenize(chunk_text, engine=\"newmm\")\n",
        "    tags = pos_tag(toks, corpus=\"orchid\")\n",
        "    verbs = [w for (w, p) in tags if p.startswith(\"V\") and w not in TH_STOP]\n",
        "    if any(w in chunk_text for w in {\"‡∏•‡∏á‡πÇ‡∏ó‡∏©\",\"‡∏™‡∏≤‡∏õ\",\"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"}): return \"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"\n",
        "    if any(w in chunk_text for w in {\"‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á\",\"‡πÑ‡∏õ‡∏´‡∏≤\",\"‡πÑ‡∏õ‡∏û‡∏ö\",\"‡∏Ç‡∏∂‡πâ‡∏ô‡∏ü‡πâ‡∏≤\",\"‡∏™‡∏≤‡∏¢‡∏£‡∏∏‡πâ‡∏á\"}): return \"‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏´‡∏≤\"\n",
        "    if any(w in chunk_text for w in {\"‡πÄ‡∏à‡∏£‡∏à‡∏≤\",\"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢\",\"‡∏ß‡∏¥‡∏á‡∏ß‡∏≠‡∏ô\",\"‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á\",\"‡∏™‡∏±‡∏ç‡∏ç‡∏≤\"}): return \"‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏à‡∏≤\"\n",
        "    if any(w in chunk_text for w in {\"‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\",\"‡πÄ‡∏´‡∏•‡πá‡∏Å‡πÉ‡∏ô\",\"‡∏°‡∏≠‡∏ö\",\"‡∏™‡∏£‡πâ‡∏≤‡∏á\",\"‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\"}): return \"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\"\n",
        "    if any(w in chunk_text for w in {\"‡∏•‡∏≥‡∏û‡∏≠‡∏á\",\"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à\",\"‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\",\"‡∏£‡∏∏‡∏°\"}): return \"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\"\n",
        "    if any(w in chunk_text for w in {\"‡∏Ç‡πÇ‡∏°‡∏¢\",\"‡∏õ‡∏•‡πâ‡∏ô\",\"‡∏ï‡∏±‡∏Å‡∏ô‡πâ‡∏≥‡∏ú‡∏∂‡πâ‡∏á\"}): return \"‡∏£‡∏±‡∏á‡∏ú‡∏∂‡πâ‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏•‡πâ‡∏ô\"\n",
        "    if verbs:\n",
        "        v = verbs[0]\n",
        "        return \"‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô\" if v.startswith(\"‡∏ï‡πà‡∏≠‡∏¢\") else f\"‡∏Å‡∏≤‡∏£{v}\"\n",
        "    kws = Counter(tokenize_for_keywords(chunk_text)).most_common(1)\n",
        "    return kws[0][0] if kws else \"‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\"\n",
        "\n",
        "def moral_line(text: str) -> str:\n",
        "    sents = split_sentences_th(text)\n",
        "    for s in sents:\n",
        "        if any(c in s for c in (\"‡∏™‡∏≠‡∏ô‡∏ß‡πà‡∏≤\",\"‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô\",\"‡∏Ñ‡∏ï‡∏¥\",\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏¥‡∏î\")):\n",
        "            return re.sub(r'^\\W+|\\W+$', '', s)\n",
        "    if any(w in text for w in {\"‡∏≠‡∏≥‡∏ô‡∏≤‡∏à\",\"‡∏•‡∏≥‡∏û‡∏≠‡∏á\",\"‡πÉ‡∏ä‡πâ‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\",\"‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î\"}):\n",
        "        return \"‡πÅ‡∏°‡πâ‡∏°‡∏µ‡∏û‡∏•‡∏±‡∏á ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏±‡∏ö‡∏¢‡∏±‡πâ‡∏á‡∏ä‡∏±‡πà‡∏á‡πÉ‡∏à ‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≤‡∏•‡∏∞‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö\"\n",
        "    return \"‡∏à‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏≠‡∏î‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à\"\n",
        "\n",
        "def summarize_rule_based(text: str) -> str:\n",
        "    text = normalize_text(text)\n",
        "    sents = split_sentences_th(text)\n",
        "    if not sents:\n",
        "        return \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ\"\n",
        "    chunks = chunk_indices(len(sents), 6)\n",
        "    scenes = []\n",
        "    for i, (st, en) in enumerate(chunks, 1):\n",
        "        sub = sents[st:en]\n",
        "        body = ensure_1_to_3_sentences(sub)\n",
        "        title = scene_title_general(\" \".join(sub)).strip(\" Ôºå,.;:!? -‚Äî\")\n",
        "        scenes.append(f\"**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà {i}: {title}**\\n{body}\\n\")\n",
        "    if len(scenes) < 6:\n",
        "        for j in range(len(scenes)+1, 7):\n",
        "            scenes.append(f\"**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà {j}: ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**\\n(‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)\\n\")\n",
        "    moral = moral_line(text)\n",
        "    return \"\\n* * *\\n\\n\".join(scenes[:6]) + f\"\\n\\n* * *\\n\\n‡∏Ñ‡∏ï‡∏¥‡∏™‡∏≠‡∏ô‡πÉ‡∏à: **{moral}**\"\n",
        "\n",
        "# ---------- LLM prompt ----------\n",
        "def build_prompt(text: str) -> str:\n",
        "    return f\"\"\"\n",
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û\n",
        "‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô \"6 ‡∏â‡∏≤‡∏Å\" (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô/‡∏Ç‡∏≤‡∏î 6 ‡∏â‡∏≤‡∏Å)\n",
        "\n",
        "‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:\n",
        "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏â‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ **2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏ï‡πá‡∏°** ‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 2 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3\n",
        "- ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö (1) ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå (2) ‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (3) ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå/‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á\n",
        "- ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
        "- ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ ‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô \"‡∏£‡∏±‡∏á‡∏ú‡∏∂‡πâ‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏•‡πâ‡∏ô\", \"‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏´‡∏≤\", \"‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏£‡∏à‡∏≤\", \"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò\", \"‡πÉ‡∏ä‡πâ‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï\", \"‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©\"\n",
        "- ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏• ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏õ‡∏µ/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô/‡∏ß‡∏±‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏\n",
        "\n",
        "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô):\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 1: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 2: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 3: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 4: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 5: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "**‡∏â‡∏≤‡∏Å‡∏ó‡∏µ‡πà 6: <‡∏ä‡∏∑‡πà‡∏≠‡∏â‡∏≤‡∏Å>**\n",
        "<‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ>\n",
        "\n",
        "* * *\n",
        "\n",
        "‡∏Ñ‡∏ï‡∏¥‡∏™‡∏≠‡∏ô‡πÉ‡∏à: <1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô>\n",
        "\n",
        "‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°:\n",
        "{text}\n",
        "\"\"\".strip()\n",
        "\n",
        "def summarize_llm(text: str, temperature: float = 0.5) -> str:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    prompt = build_prompt(text)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏ñ‡∏µ‡∏û‡∏¥‡∏ñ‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def summarize_story(text: str) -> str:\n",
        "    if USE_LLM:\n",
        "        try:\n",
        "            return summarize_llm(text)\n",
        "        except Exception as e:\n",
        "\n",
        "            return summarize_rule_based(text) + f\"\\n\\n(‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ä‡πâ Rule-based ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ LLM error: {e})\"\n",
        "    else:\n",
        "        return summarize_rule_based(text)\n",
        "\n",
        "# ---------- Batch I/O ----------\n",
        "def load_stories(json_path: str) -> List[Dict[str, Any]]:\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, dict) and \"stories\" in data and isinstance(data[\"stories\"], list):\n",
        "        items = data[\"stories\"]\n",
        "    elif isinstance(data, list):\n",
        "        items = data\n",
        "    else:\n",
        "        raise ValueError(\"‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ key 'stories'\")\n",
        "    # normalize fields\n",
        "    norm = []\n",
        "    for i, it in enumerate(items):\n",
        "        text = it.get(\"text\") or it.get(\"content\") or it.get(\"story\") or \"\"\n",
        "        if not text or not str(text).strip():\n",
        "            continue\n",
        "        norm.append({\n",
        "            \"id\": it.get(\"id\", f\"story_{i+1:04d}\"),\n",
        "            \"title\": it.get(\"title\", \"\"),\n",
        "            \"text\": str(text),\n",
        "            **{k: v for k, v in it.items() if k not in {\"id\",\"title\",\"text\"}},\n",
        "        })\n",
        "    if not norm:\n",
        "        raise ValueError(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ü‡∏¥‡∏•‡∏î‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (text/content/story) ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏î‡πÄ‡∏•‡∏¢\")\n",
        "    return norm\n",
        "\n",
        "def batch_summarize_json(input_json: str,\n",
        "                         out_json: str = \"summaries.json\",\n",
        "                         out_csv: str = \"summaries.csv\",\n",
        "                         sleep_sec: float = 0.0) -> pd.DataFrame:\n",
        "    items = load_stories(input_json)\n",
        "    results = []\n",
        "    for idx, it in enumerate(items, 1):\n",
        "        story_id = it[\"id\"]\n",
        "        title = it.get(\"title\", \"\")\n",
        "        text  = it[\"text\"]\n",
        "        print(f\"[{idx}/{len(items)}] summarizing: {story_id} {('('+title+')') if title else ''}\")\n",
        "        summary = summarize_story(text)\n",
        "        results.append({\n",
        "            \"id\": story_id,\n",
        "            \"title\": title,\n",
        "            \"summary_6scenes\": summary\n",
        "        })\n",
        "        if sleep_sec > 0:\n",
        "            time.sleep(sleep_sec)\n",
        "    # save JSON\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    # save CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"‚úÖ Done. Saved: {out_json}, {out_csv}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_path = \"/content/drive/MyDrive/NLP_DL data/21-40stories_fixed.json\"       # <-- ‡πÉ‡∏™‡πà path ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "    out_json  = \"21-40summaries.json\"\n",
        "    out_csv   = \"21-40summaries.csv\"\n",
        "\n",
        "    df = batch_summarize_json(input_path, out_json, out_csv, sleep_sec=0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoOSLHgFPfT_",
        "outputId": "5db66495-ec96-4a34-b687-b1750b246b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/20] summarizing: story_0001 (tale21)\n",
            "[2/20] summarizing: story_0002 (tale22)\n",
            "[3/20] summarizing: story_0003 (tale23)\n",
            "[4/20] summarizing: story_0004 (tale24)\n",
            "[5/20] summarizing: story_0005 (tale25)\n",
            "[6/20] summarizing: story_0006 (tale26)\n",
            "[7/20] summarizing: story_0007 (tale27)\n",
            "[8/20] summarizing: story_0008 (tale28)\n",
            "[9/20] summarizing: story_0009 (tale29)\n",
            "[10/20] summarizing: story_0010 (tale30)\n",
            "[11/20] summarizing: story_0011 (tale31)\n",
            "[12/20] summarizing: story_0012 (tale32)\n",
            "[13/20] summarizing: story_0013 (tale33)\n",
            "[14/20] summarizing: story_0014 (tale34)\n",
            "[15/20] summarizing: story_0015 (tale35)\n",
            "[16/20] summarizing: story_0016 (tale36)\n",
            "[17/20] summarizing: story_0017 (tale37)\n",
            "[18/20] summarizing: story_0018 (tale38)\n",
            "[19/20] summarizing: story_0019 (tale39)\n",
            "[20/20] summarizing: story_0020 (tale40)\n",
            "‚úÖ Done. Saved: 21-40summaries.json, 21-40summaries.csv\n"
          ]
        }
      ]
    }
  ]
}